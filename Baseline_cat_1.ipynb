{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【概要】\n",
    "\n",
    "- モデル：Single LightGBM\n",
    "- 特徴量：n個　（メモリが足りないから、全て別のノートブックで作成し、inputから使う）\n",
    "- Local CV：\n",
    "- Public LB："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 23.7 s, total: 2min 19s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from google.cloud import storage as gcs\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "project_id = \"Project kaggle\"\n",
    "bucket_name = \"kaggle_riiid_ynum\"\n",
    "gcs_path = \"input/train.csv\"\n",
    "\n",
    "client = gcs.Client(project_id)\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# GCSにあるダウンロードしたいファイルを指定\n",
    "blob = bucket.blob(gcs_path)\n",
    "\n",
    "# DataFrameとして受け取る(ヘッダーありのCSVを想定)\n",
    "train_df = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "# 型の変換\n",
    "# data_types_dict = {\n",
    "#     'timestamp': 'int64',\n",
    "#     'user_id': 'int32', \n",
    "#     'content_id': 'int16', \n",
    "#     'content_type_id':'int8', \n",
    "#     'task_container_id': 'int16',\n",
    "#     'user_answer': 'int8',\n",
    "#     'answered_correctly': 'int8', \n",
    "#     'prior_question_elapsed_time': 'float32', \n",
    "#     'prior_question_had_explanation': 'bool'\n",
    "# }\n",
    "\n",
    "# train_df = train_df.astype(data_types_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 型の変換\n",
    "data_types_dict = {\n",
    "    'timestamp': 'int64',\n",
    "    'user_id': 'int32', \n",
    "    'content_id': 'int16', \n",
    "    'content_type_id':'int8', \n",
    "    'task_container_id': 'int16',\n",
    "    'user_answer': 'int8',\n",
    "    'answered_correctly': 'int8', \n",
    "    'prior_question_elapsed_time': 'float32', \n",
    "    'prior_question_had_explanation': 'bool'\n",
    "}\n",
    "\n",
    "train_df = train_df.astype(data_types_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory\n",
    "# %cd /Users/aoi/Documents/kaggle/riiid/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpuを使用するとき\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "# import eli5\n",
    "# from eli5.sklearn import PermutationImportance\n",
    "# import datatable as dt\n",
    "import lightgbm as lgb\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "# import riiideducation\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.metrics as metrics\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "#models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb\n",
    "from catboost import Pool\n",
    "from sklearn.linear_model import RidgeCV, RidgeClassifierCV\n",
    "#HDKIM SAKT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "_ = np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "#列を…で省略させずに、全部表示(1000列上限にしてる)させるための設定\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "def seed_everything(seed=20):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(seed=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 20.3 ms, total: 1.14 s\n",
      "Wall time: 1.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#各列のデータタイプを辞書型で指定し、data_types_dictに保存\n",
    "\n",
    "# data_types_dict = {\n",
    "#     'timestamp': 'int64',\n",
    "#     'user_id': 'int32', \n",
    "#     'content_id': 'int16', \n",
    "#     'content_type_id':'int8', \n",
    "#     'task_container_id': 'int16',\n",
    "#     'user_answer': 'int8',\n",
    "#     'answered_correctly': 'int8', \n",
    "#     'prior_question_elapsed_time': 'float32', \n",
    "#     'prior_question_had_explanation': 'bool'\n",
    "# }\n",
    "\n",
    "target = 'answered_correctly'\n",
    "\n",
    "# train_df = dt.fread('../input/riiid-test-answer-prediction/train.csv', columns=set(data_types_dict.keys())).to_pandas()\n",
    "# train_df = pd.read_pickle('./input/train.pkl')\n",
    "\n",
    "train_df_use_cols = [\n",
    "#     'row_id',\n",
    "    'timestamp',\n",
    "    'user_id',\n",
    "    'content_id',\n",
    "#     'content_type_id',\n",
    "    'task_container_id',\n",
    "#     'user_answer',\n",
    "    'answered_correctly',\n",
    "    'prior_question_elapsed_time',\n",
    "    'prior_question_had_explanation'\n",
    "]\n",
    "\n",
    "train_df = train_df[train_df_use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df[target] != -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainに特徴量を結合\n",
    "\n",
    "ただ、作成した特徴量は\n",
    "\n",
    "①train_dfと同じ行数ある特徴量（cum系）\n",
    "\n",
    "②なんかのidをキーとしてtrain_dfとleft joinする特徴量\n",
    "\n",
    "があり、ここでは①を結合。②はメモリの事情でtrainとvalidに分割した後でそれぞれに結合させる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainは約1億行あって重いから、ランダムに「train_mini_len行」抽出しtrain_miniにいれる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mini_len = 2*10*1000*1000\n",
    "# train_mini_len = len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mini = train_df.sample(n=train_mini_len, random_state=42)\n",
    "# train_mini = train_df.copy()\n",
    "\n",
    "del train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000000, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mini.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_user_ans.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "# feature = feature[['user_cnt', 'user_corcnt', 'user_inccnt', 'user_corrate', 'user_diff_sum_corrate', 'user_diff_avg_corrate']]\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_user_task.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "feature = feature[['task_container_id_back']]\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_user_roll.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "feature = feature[['user_roll10_corrate', 'user_roll30_corrate']]\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_user_time.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "feature = feature[['user_diff_sum_time', 'user_diff_avg_time']]\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_user_past.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "feature = feature[['user_past_content_cnt']]\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_user_part_ans.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_user_part_diff_corrate.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_user_part_roll.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "feature = feature[['user_part_roll5_corrate', 'user_part_roll15_corrate']]\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_user_w2v.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "feature = feature[['user_w2v_corrate']]\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_user_tags1_ans.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "feature = feature[['user_tags1_corcnt', 'user_tags1_inccnt' ,'user_tags1_corrate']]\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_013_cum.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_014_cum.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'features/fe_015_cum.csv'\n",
    "blob = bucket.blob(path)\n",
    "feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "\n",
    "feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "del feature\n",
    "\n",
    "train_mini = pd.concat([train_mini, feature_mini], axis=1)\n",
    "del feature_mini\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "別のノートブックで作成した特徴量同士を結合（concat）して１つのデータフレーム（fe）を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 特徴量のpickleファイルがあるパスを指定\n",
    "# fe_user_ans = 'features/fe_user_ans.csv'\n",
    "# fe_user_task = 'features/fe_user_task.csv'\n",
    "# fe_user_roll = 'features/fe_user_roll.csv'\n",
    "# fe_user_time = 'features/fe_user_time.csv'\n",
    "# fe_user_past = 'features/fe_user_past.csv'\n",
    "# fe_user_part_ans = 'features/fe_user_part_ans.csv'\n",
    "# fe_user_part_diff_corrate = 'features/fe_user_part_diff_corrate.csv'\n",
    "# fe_user_part_roll = 'features/fe_user_part_roll.csv'\n",
    "# fe_user_tags1_ans = 'features/fe_user_tags1_ans.csv'\n",
    "# fe_user_w2v = 'features/fe_user_w2v.csv'\n",
    "# fe_013_cum = 'features/fe_013_cum.csv'\n",
    "# fe_014_cum = 'features/fe_014_cum.csv'\n",
    "# fe_015_cum = 'features/fe_015_cum.csv'\n",
    "\n",
    "# path_list = [\n",
    "#     fe_user_ans,\n",
    "#     fe_user_task,\n",
    "#     fe_user_roll,\n",
    "#     fe_user_time,\n",
    "#     fe_user_past,\n",
    "#     fe_user_part_ans,\n",
    "#     fe_user_part_diff_corrate,\n",
    "#     fe_user_part_roll,\n",
    "#     fe_user_tags1_ans,\n",
    "#     fe_user_w2v,\n",
    "#     fe_013_cum,\n",
    "#     fe_014_cum,\n",
    "#     fe_015_cum\n",
    "# ]\n",
    "\n",
    "# #特徴量だけのデータフレームを作成\n",
    "# fe = pd.DataFrame()\n",
    "\n",
    "# for path in path_list:\n",
    "    \n",
    "#     # GCSにあるダウンロードしたいファイルを指定\n",
    "#     blob = bucket.blob(path)\n",
    "\n",
    "#     # DataFrameとして受け取る(ヘッダーありのCSVを想定)\n",
    "#     feature = pd.read_csv(BytesIO(blob.download_as_string()))\n",
    "# #     feature = pd.read_pickle(path)\n",
    "\n",
    "#     feature_mini = feature.sample(n=train_mini_len, random_state=42)\n",
    "\n",
    "# #     fe = pd.concat([fe, feature], axis=1)\n",
    "\n",
    "#     del feature\n",
    "#     gc.collect()\n",
    "    \n",
    "#     fe = pd.concat([fe, feature_mini], axis=1)\n",
    "    \n",
    "#     del feature_mini\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_miniとfeの結合（concat）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mini = pd.concat([train_mini, fe], axis=1)\n",
    "\n",
    "# del fe\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000000, 38)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mini.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_miniをtrainとvalidに分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_miniの中からvalidの新規ユーザーとして使うユーザーをランダムに選ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = train_mini['user_id'].drop_duplicates()\n",
    "users = users.sample(frac=0.025, random_state=42)\n",
    "\n",
    "users_df = pd.DataFrame()\n",
    "users_df['user_id'] = users.values\n",
    "\n",
    "valid_df_newuser = pd.merge(train_mini, users_df, on=['user_id'], how='inner',right_index=True)\n",
    "\n",
    "del users_df\n",
    "del users\n",
    "gc.collect()\n",
    "\n",
    "train_mini.drop(valid_df_newuser.index, inplace=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_miniとvalid_df_newuserそれぞれに②の特徴量を結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量のpickleファイルがあるパスを指定\n",
    "path = 'features/fe_content.csv'\n",
    "blob = bucket.blob(path)\n",
    "features = pd.read_csv(BytesIO(blob.download_as_string()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[[\n",
    "    'bundle_id',\n",
    "    'part',\n",
    "    'content_cnt',\n",
    "#         'content_corcnt',\n",
    "    'content_inccnt',\n",
    "    'content_corrate',\n",
    "#         'content_corstd',\n",
    "#         'content_sum_time',\n",
    "    'content_avg_time',\n",
    "#         'content_expl_sum',\n",
    "    'content_expl_avg',\n",
    "#         'content_lecture_cnt',\n",
    "#         'content_lecture_sum_time',\n",
    "#         'content_lecture_avg_time',\n",
    "\n",
    "#         'new_content_cnt',\n",
    "    'new_content_corcnt',\n",
    "#         'new_content_inccnt',\n",
    "    'new_content_corrate',\n",
    "\n",
    "#         'bundle_cnt',\n",
    "    'same_bundle_cnt',\n",
    "#         'bundle_unicnt',\n",
    "#         'bundle_corcnt',\n",
    "#         'bundle_inccnt',\n",
    "    'bundle_corrate',\n",
    "#         'bundle_corstd',\n",
    "\n",
    "#         'part_cnt',\n",
    "#         'part_corcnt',\n",
    "#         'part_inccnt',\n",
    "#         'part_corrate',\n",
    "#         'part_corstd',\n",
    "\n",
    "#         'cluster',\n",
    "#         'cluster_cnt',\n",
    "#         'cluster_corcnt',\n",
    "#         'cluster_inccnt',\n",
    "#         'cluster_corrate',\n",
    "#         'cluster_corstd',\n",
    "\n",
    "    'tags1',\n",
    "#         'tags1_cnt',\n",
    "#         'tags1_corcnt',\n",
    "#         'tags1_inccnt',\n",
    "#         'tags1_corrate',\n",
    "#         'tags1_corstd',\n",
    "    'tags2',\n",
    "#         'tags3',\n",
    "#         'tags4',\n",
    "#         'tags5',\n",
    "#         'tags6',\n",
    "\n",
    "#         'le_tags',\n",
    "    'new_w2v',\n",
    "#     'w2v',\n",
    "\n",
    "#         'question_had_explanation_mean',\n",
    "#         'tags_lsi',\n",
    "#         'tag_acc_count',\n",
    "#         'tag_acc_max',\n",
    "#         'tag_acc_min',  \n",
    "]]\n",
    "#     feature = feature.drop(columns='content_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mini = pd.concat([train_mini.reset_index(drop=True), features.reindex(train_mini['content_id'].values).reset_index(drop=True)], axis=1)\n",
    "valid_df_newuser = pd.concat([valid_df_newuser.reset_index(drop=True), features.reindex(valid_df_newuser['content_id'].values).reset_index(drop=True)], axis=1)\n",
    "    \n",
    "# del feature\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17755881, 52)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mini.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valid_dfとvalid_df_newuserを結合してvalid完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df = train_mini.sample(frac=0.09, random_state=42)\n",
    "train_mini.drop(valid_df.index, inplace=True)\n",
    "\n",
    "valid_df = valid_df.append(valid_df_newuser)\n",
    "\n",
    "del valid_df_newuser\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainとvalidが何行何列か確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainの行数と列数： (17755881, 52)\n",
      "validの行数と列数： (2244119, 52)\n"
     ]
    }
   ],
   "source": [
    "print('trainの行数と列数：', train_mini.shape)\n",
    "print('validの行数と列数：', valid_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴量に使う列名をfeaturesに入れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \n",
    "### デフォルトで存在する特徴量 ###\n",
    "#     'user_id',\n",
    "    'content_id',\n",
    "    'timestamp',\n",
    "    'task_container_id',\n",
    "    'prior_question_elapsed_time',\n",
    "    'prior_question_had_explanation',\n",
    "    'bundle_id',\n",
    "    'part',    \n",
    "    \n",
    "### 問題に関する特徴量 ###\n",
    "    \n",
    "    'content_cnt',\n",
    "#     'content_corcnt',\n",
    "    'content_inccnt',\n",
    "    'content_corrate',\n",
    "#     'content_corstd',\n",
    "#     'content_sum_time',\n",
    "    'content_avg_time',\n",
    "#     'content_expl_sum',\n",
    "    'content_expl_avg',\n",
    "#     'content_lecture_cnt',\n",
    "#     'content_lecture_sum_time',\n",
    "#     'content_lecture_avg_time',\n",
    "    \n",
    "#     'new_content_cnt',\n",
    "    'new_content_corcnt',\n",
    "#     'new_content_inccnt',\n",
    "    'new_content_corrate',\n",
    "    \n",
    "#     'bundle_cnt',\n",
    "    'same_bundle_cnt',\n",
    "#     'bundle_unicnt',\n",
    "#     'bundle_corcnt',\n",
    "#     'bundle_inccnt',\n",
    "    'bundle_corrate',\n",
    "#     'bundle_corstd',\n",
    "\n",
    "#     'part_cnt',\n",
    "#     'part_corcnt',\n",
    "#     'part_inccnt',\n",
    "#     'part_corrate',\n",
    "#     'part_corstd',\n",
    "    \n",
    "#     'cluster',\n",
    "#     'cluster_cnt',\n",
    "#     'cluster_corcnt',\n",
    "#     'cluster_inccnt',\n",
    "#     'cluster_corrate',\n",
    "#     'cluster_corstd',\n",
    "    \n",
    "    'tags1',\n",
    "#     'tags1_cnt',\n",
    "#     'tags1_corcnt',\n",
    "#     'tags1_inccnt',\n",
    "#     'tags1_corrate',\n",
    "#     'tags1_corstd',\n",
    "    'tags2',\n",
    "#     'tags3',\n",
    "#     'tags4',\n",
    "#     'tags5',\n",
    "#     'tags6',\n",
    "    \n",
    "#     'le_tags',\n",
    "    'new_w2v',\n",
    "#     'w2v',\n",
    "    \n",
    "#     'question_had_explanation_mean',\n",
    "#     'tags_lsi',\n",
    "#     'tag_acc_count',\n",
    "#     'tag_acc_max',\n",
    "#     'tag_acc_min',\n",
    "    \n",
    "### ユーザーに関する特徴量 ###\n",
    "    \n",
    "#     'timestamp_rank',\n",
    "#     'timestamp_diff',\n",
    "    \n",
    "#     'frequency',\n",
    "    'elapsed_time_u_avg',\n",
    "    'explanation_u_avg',\n",
    "    'timestamp_u_correct_recency',\n",
    "    'timestamp_u_incorrect_recency',\n",
    "    'timestamp_u_diff_0_1',\n",
    "    'timestamp_u_diff_1_2',\n",
    "    'timestamp_u_diff_2_3',\n",
    "    \n",
    "#     'task_container_id_max',\n",
    "    'task_container_id_back',\n",
    "    \n",
    "    'user_cnt',\n",
    "    'user_corcnt',\n",
    "    'user_inccnt',\n",
    "    'user_corrate',\n",
    "    \n",
    "    'user_diff_sum_corrate',\n",
    "    'user_diff_avg_corrate',\n",
    "    \n",
    "#     'user_roll3_corrate',\n",
    "#     'user_roll5_corrate',\n",
    "    'user_roll10_corrate',\n",
    "#     'user_roll20_corrate',\n",
    "    'user_roll30_corrate',\n",
    "#     'user_roll50_corrate',\n",
    "    \n",
    "#     'user_sum_time',\n",
    "#     'user_avg_time',\n",
    "    'user_diff_sum_time',\n",
    "    'user_diff_avg_time',\n",
    "    \n",
    "#     'user_expl_avg',\n",
    "    \n",
    "#     'diagnostic_test',\n",
    "#     'diagnostic_test_2',\n",
    "    \n",
    "    'user_past_content_cnt',\n",
    "#     'user_past_expl_cnt',\n",
    "#     'user_past_lecture_cnt',\n",
    "    \n",
    "    'user_part_cnt',\n",
    "    'user_part_corcnt', \n",
    "    'user_part_inccnt',\n",
    "    'user_part_corrate',\n",
    "    \n",
    "    'user_part_diff_sum_corrate',\n",
    "    'user_part_diff_avg_corrate',\n",
    "    \n",
    "#     'user_part_roll3_corrate',\n",
    "    'user_part_roll5_corrate',\n",
    "#     'user_part_roll10_corrate',\n",
    "    'user_part_roll15_corrate',\n",
    "#     'user_part_roll20_corrate',\n",
    "#     'user_part_roll30_corrate',\n",
    "    \n",
    "#     'user_w2v_cnt',\n",
    "#     'user_w2v_corcnt',\n",
    "#     'user_w2v_inccnt',\n",
    "    'user_w2v_corrate',\n",
    "    \n",
    "#     'user_tags1_cnt',\n",
    "    'user_tags1_corcnt',\n",
    "    'user_tags1_inccnt',\n",
    "    'user_tags1_corrate'\n",
    "]\n",
    "\n",
    "# カテゴリ変数\n",
    "categorical_columns = [\n",
    "    \n",
    "### デフォルトで存在する特徴量 ###\n",
    "#     'user_id',\n",
    "    'content_id',\n",
    "#     'timestamp',\n",
    "#     'task_container_id',\n",
    "    'prior_question_had_explanation',\n",
    "    'bundle_id',\n",
    "    'part',    \n",
    "    \n",
    "### 問題に関する特徴量 ###\n",
    "    \n",
    "    'same_bundle_cnt',\n",
    "    \n",
    "#     'cluster',\n",
    "\n",
    "    'tags1',\n",
    "    'tags2',\n",
    "#     'tags3',\n",
    "#     'tags4',\n",
    "#     'tags5',\n",
    "#     'tags6',\n",
    "    'new_w2v',\n",
    "#     'w2v',\n",
    "#     'tags_lsi',\n",
    "    \n",
    "### ユーザーに関する特徴量 ###\n",
    "    \n",
    "#     'timestamp_rank',\n",
    "    \n",
    "#     'task_container_id_max',\n",
    "    'task_container_id_back',\n",
    "    \n",
    "#     'diagnostic_test',\n",
    "#     'diagnostic_test_2',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徴量の個数： 50 個\n"
     ]
    }
   ],
   "source": [
    "print('特徴量の個数：', len(features), '個')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# '''\n",
    "#エラー起こらないように型を変える\n",
    "train_mini['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "valid_df['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "\n",
    "train_mini['prior_question_had_explanation'] = train_mini['prior_question_had_explanation'].astype(bool)\n",
    "valid_df['prior_question_had_explanation'] = valid_df['prior_question_had_explanation'].astype(bool)\n",
    "\n",
    "train_mini['prior_question_had_explanation'] = train_mini['prior_question_had_explanation'].astype(int)\n",
    "valid_df['prior_question_had_explanation'] = valid_df['prior_question_had_explanation'].astype(int)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# データが巨大で時間かかるので省略\n",
    "'''\n",
    "for col in features:\n",
    "    try:\n",
    "        plt.title(col)\n",
    "        sns.distplot(train_mini[col].fillna(0))#.sample(1000000, random_state=42)\n",
    "        plt.show()\n",
    "    except:\n",
    "        print('='*30)\n",
    "        print(f'ERROR IN {col}')\n",
    "        print('='*30)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量を正規分布に近い形に変形する(対数変換など)\n",
    "- スコアが上がることがある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 右裾が長い特徴量（対数変換）\n",
    "# log_features = [\n",
    "#     'timestamp',\n",
    "#     'task_container_id',\n",
    "#     'prior_question_elapsed_time',\n",
    "#     'content_cnt',\n",
    "#     'content_corcnt',\n",
    "#     'content_sum_time',\n",
    "#     'content_avg_time',\n",
    "#     'content_expl_sum',\n",
    "#     'content_lecture_cnt',\n",
    "#     'content_lecture_sum_time',\n",
    "#     'content_lecture_avg_time',\n",
    "#     'new_content_cnt',\n",
    "#     'new_content_corcnt',\n",
    "#     'new_content_inccnt',\n",
    "#     'bundle_cnt',\n",
    "#     'bundle_unicnt',\n",
    "#     'bundle_corcnt',\n",
    "#     'bundle_inccnt',\n",
    "#     'timestamp_rank',\n",
    "#     'timestamp_diff',\n",
    "#     'task_container_id_max',\n",
    "#     'user_cnt',\n",
    "#     'user_corcnt',\n",
    "#     'user_inccnt',\n",
    "#     'user_sum_time',\n",
    "#     'user_avg_time',\n",
    "#     'user_part_cnt',\n",
    "#     'user_part_corcnt',\n",
    "#     'user_part_inccnt',\n",
    "#     'user_tags1_cnt',\n",
    "#     'user_tags1_corcnt',\n",
    "#     'user_tags1_inccnt'\n",
    "# ]\n",
    "\n",
    "# # 右に裾が長い特徴量（ルート変換）\n",
    "# sqrt_features = [\n",
    "    \n",
    "# ]\n",
    "\n",
    "# # 左に裾が長い特徴量（未解決）\n",
    "# left_long_features = [\n",
    "#     'content_corstd',\n",
    "#     'content_expl_avg',\n",
    "#     'bundle_corstd'\n",
    "# ]\n",
    "\n",
    "# # 両側に裾が長い特徴量（絶対値対数変換）\n",
    "# abs_log_features = [\n",
    "\n",
    "# ]\n",
    "\n",
    "# # 両側に裾が長い特徴量（絶対値ルート変換）\n",
    "# abs_sqrt_features = [\n",
    "#     'user_diff_sum_corrate',\n",
    "#     'user_diff_sum_time',\n",
    "#     'user_part_diff_sum_corrate'\n",
    "# ]\n",
    "\n",
    "# # 外れ値を置き換える（clipping）特徴量\n",
    "# clip_features = [\n",
    "#     'user_diff_avg_time'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 対数変換\n",
    "# for col in log_features:\n",
    "#     train_mini[col] = np.log1p(train_mini[col])\n",
    "#     valid_df[col] = np.log1p(valid_df[col])\n",
    "\n",
    "# # ルート変換\n",
    "# for col in sqrt_features:\n",
    "#     train_mini[col] = np.sqrt(train_mini[col])\n",
    "#     valid_df[col] = np.sqrt(valid_df[col])\n",
    "\n",
    "# # 絶対値対数変換\n",
    "# for col in abs_log_features:\n",
    "#     train_mini[col] = np.sign(train_mini[col])*np.log(np.abs(train_mini[col]))\n",
    "#     valid_df[col] = np.sign(valid_df[col])*np.log(np.abs(valid_df[col]))\n",
    "\n",
    "# # 絶対値ルート変換\n",
    "# for col in abs_sqrt_features:\n",
    "#     train_mini[col] = np.sign(train_mini[col])*np.sqrt(np.abs(train_mini[col]))\n",
    "#     valid_df[col] = np.sign(valid_df[col])*np.sqrt(np.abs(valid_df[col]))\n",
    "\n",
    "# # 外れ値を置き換える（clipping）\n",
    "# for col in abs_sqrt_features:\n",
    "\n",
    "#     col_all = train_mini[col] # + valid_df[col]\n",
    "                \n",
    "#     p01 = col_all.quantile(0.01)\n",
    "#     p99 = col_all.quantile(0.99)\n",
    "\n",
    "#     train_mini[col] = train_mini[col].clip(p01, p99)                                            \n",
    "#     valid_df[col] = valid_df[col].clip(p01, p99)\n",
    "\n",
    "#     del col_all   \n",
    "\n",
    "# gc.collect()                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features再度確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "'''\n",
    "for col in features:\n",
    "    try:\n",
    "        plt.title(col)\n",
    "        sns.distplot(train_mini[col].fillna(0))#.sample(1000000, random_state=42)\n",
    "        plt.show()\n",
    "    except:\n",
    "        print('='*30)\n",
    "        print(f'ERROR IN {col}')\n",
    "        print('='*30)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### わかりやすくする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#わかりやすく置き換えているだけ\n",
    "tr = train_mini\n",
    "del train_mini\n",
    "val = valid_df\n",
    "del valid_df\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "feature_cols = features\n",
    "cat_feature_cols = categorical_columns\n",
    "num_feature_cols = list(set(feature_cols) - set(cat_feature_cols))\n",
    "\n",
    "target_col = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = tr[tr['answered_correctly'].notnull()]\n",
    "val = val[val['answered_correctly'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_xgb(train, valid, feature_cols, target, num_iter=10000):\n",
    "\n",
    "#     xgb_params = {\n",
    "#         'objective':'binary:logistic',\n",
    "#         'eval_metric':'auc',\n",
    "#         'eta':0.01,\n",
    "#         'seed':42,\n",
    "#         'max_depth':10,\n",
    "#         'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'auto',\n",
    "#     }\n",
    "\n",
    "#     train_set = xgb.DMatrix(train[feature_cols], train[target])\n",
    "#     valid_set = xgb.DMatrix(valid[feature_cols], valid[target])\n",
    "\n",
    "#     xgb_model = xgb.train(xgb_params, train_set, \n",
    "#                     evals = [(train_set, 'train'),(valid_set, 'valid')],\n",
    "#                     num_boost_round = num_iter,\n",
    "#                     early_stopping_rounds=10,\n",
    "#                     verbose_eval=100)\n",
    "    \n",
    "#     pred = xgb_model.predict(xgb.DMatrix(valid[feature_cols]))\n",
    "#     valid_auc = metrics.roc_auc_score(valid[target_col], pred)\n",
    "\n",
    "#     return valid_auc, xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_lgbm(train, valid, feature_cols, categorical_feature_cols, target, num_iter=10000):\n",
    "    \n",
    "#     lgb_params = {\n",
    "#     'num_leaves': 350,\n",
    "#     'max_bin':700,\n",
    "#     'min_child_weight': 0.03454472573214212,\n",
    "#     'feature_fraction': 0.58,\n",
    "#     'bagging_fraction': 0.58,\n",
    "#     #'min_data_in_leaf': 106,\n",
    "#     'objective': 'binary',\n",
    "#     'max_depth': -1,\n",
    "#     'learning_rate': 0.01,\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"bagging_seed\": 11,\n",
    "#     \"metric\": 'auc',\n",
    "#     \"verbosity\": -1,\n",
    "#     'reg_alpha': 0.3899927210061127,\n",
    "#     'reg_lambda': 0.6485237330340494,\n",
    "#     'random_state': 47\n",
    "#     }\n",
    "    \n",
    "#     train_set = lgb.Dataset(train[feature_cols], train[target])\n",
    "#     valid_set = lgb.Dataset(valid[feature_cols], valid[target])\n",
    "\n",
    "#     lgb_model = lgb.train(lgb_params, train_set,\n",
    "#                     categorical_feature=categorical_feature_cols,\n",
    "#                     valid_sets=[train_set, valid_set],\n",
    "#                     valid_names = ['train','valid'],\n",
    "#                     num_boost_round = num_iter,\n",
    "#                     early_stopping_rounds=10,\n",
    "#                     verbose_eval=100)\n",
    "\n",
    "#     #pred = lgb_model.predict(train[feature_cols])\n",
    "#     #train_loss = metrics.roc_auc_score(train[target_col], pred)\n",
    "#     pred = lgb_model.predict(valid[feature_cols])\n",
    "#     valid_loss = metrics.roc_auc_score(valid[target], pred)\n",
    "#     #重要度\n",
    "#     feature_imp = pd.DataFrame(sorted(zip(lgb_model.feature_importance(), feature_cols)), columns=['Value','Feature'])\n",
    "#     plt.figure(figsize=(20, 10))\n",
    "#     sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "#     plt.show()\n",
    "    \n",
    "#     return valid_loss, lgb_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ctb(train, valid, feature_cols, target, num_iter=100000):\n",
    "    \n",
    "    ctb_params = {\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'AUC',\n",
    "        'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n",
    "#        'grow_policy': 'Lossguide',\n",
    "        'iterations': num_iter,\n",
    "        'learning_rate': 0.01,\n",
    "        'random_seed': 42,\n",
    "#        'l2_leaf_reg': 1e-1,\n",
    "        'depth': 9,\n",
    "#        'max_leaves': 10,\n",
    "#        'border_count': 128,\n",
    "        'verbose': 100,\n",
    "        'early_stopping_rounds': 10,\n",
    "    }\n",
    "    \n",
    "    train_set = Pool(train[feature_cols], train[target])\n",
    "    valid_set = Pool(valid[feature_cols], valid[target])\n",
    "    \n",
    "    ctb_model = ctb.CatBoost(ctb_params)\n",
    "    ctb_model.fit(train_set, eval_set=[valid_set], use_best_model=True)\n",
    "    #pred = ctb_model.predict(valid_set, prediction_type='Class')\n",
    "    valid_auc = ctb_model.get_best_score()['validation']['AUC']\n",
    "    \n",
    "    return valid_auc, ctb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# target = 'answered_correctly'\n",
    "\n",
    "# # train_df = dt.fread('../input/riiid-test-answer-prediction/train.csv', columns=set(data_types_dict.keys())).to_pandas()\n",
    "\n",
    "# train_df = pd.read_pickle('./input/train.pkl')\n",
    "\n",
    "# train_df_use_cols = [\n",
    "# #     'row_id',\n",
    "#     'timestamp',\n",
    "#     'user_id',\n",
    "#     'content_id',\n",
    "#     'content_type_id',\n",
    "#     'task_container_id',\n",
    "# #     'user_answer',\n",
    "#     'answered_correctly',\n",
    "# #     'prior_question_elapsed_time',\n",
    "# #     'prior_question_had_explanation'\n",
    "# ]\n",
    "\n",
    "# train_df = train_df[train_df_use_cols]\n",
    "\n",
    "# train_df = train_df[train_df[target] != -1].reset_index(drop=True)\n",
    "# train_df = train_df[train_df.content_type_id == False]\n",
    "# train_df = train_df.sort_values(['timestamp'], ascending=True)\n",
    "# train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_SAMPLES = 320000\n",
    "# MAX_SEQ = 150\n",
    "# MIN_SAMPLES = 5\n",
    "# EMBED_DIM = 128\n",
    "# DROPOUT_RATE = 0.2\n",
    "# LEARNING_RATE = 1e-3\n",
    "# MAX_LEARNING_RATE = 1e-2\n",
    "# EPOCHS = 50\n",
    "# TRAIN_BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skills = train_df[\"content_id\"].unique()\n",
    "# #joblib.dump(skills, \"skills.pkl.zip\")\n",
    "# n_skill = len(skills)\n",
    "# print(\"number skills\", len(skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group = train_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n",
    "#             r['content_id'].values,\n",
    "#             r['answered_correctly'].values))\n",
    "\n",
    "# joblib.dump(group, \"group.pkl.zip\")\n",
    "# del train_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SAKTDataset(Dataset):\n",
    "#     def __init__(self, group, n_skill, min_samples=1, max_seq=128):\n",
    "#         super(SAKTDataset, self).__init__()\n",
    "#         self.max_seq = max_seq\n",
    "#         self.n_skill = n_skill\n",
    "#         self.samples = {}\n",
    "        \n",
    "#         self.user_ids = []\n",
    "#         for user_id in group.index:\n",
    "#             q, qa = group[user_id]\n",
    "#             if len(q) < min_samples:\n",
    "#                 continue\n",
    "            \n",
    "#             # Main Contribution\n",
    "#             if len(q) > self.max_seq:\n",
    "#                 total_questions = len(q)\n",
    "#                 initial = total_questions % self.max_seq\n",
    "#                 if initial >= min_samples:\n",
    "#                     self.user_ids.append(f\"{user_id}_0\")\n",
    "#                     self.samples[f\"{user_id}_0\"] = (q[:initial], qa[:initial])\n",
    "#                 for seq in range(total_questions // self.max_seq):\n",
    "#                     self.user_ids.append(f\"{user_id}_{seq+1}\")\n",
    "#                     start = initial + seq * self.max_seq\n",
    "#                     end = start + self.max_seq\n",
    "#                     self.samples[f\"{user_id}_{seq+1}\"] = (q[start:end], qa[start:end])\n",
    "#             else:\n",
    "#                 user_id = str(user_id)\n",
    "#                 self.user_ids.append(user_id)\n",
    "#                 self.samples[user_id] = (q, qa)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.user_ids)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         user_id = self.user_ids[index]\n",
    "#         q_, qa_ = self.samples[user_id]\n",
    "#         seq_len = len(q_)\n",
    "\n",
    "#         q = np.zeros(self.max_seq, dtype=int)\n",
    "#         qa = np.zeros(self.max_seq, dtype=int)\n",
    "#         if seq_len == self.max_seq:\n",
    "#             q[:] = q_\n",
    "#             qa[:] = qa_\n",
    "#         else:\n",
    "#             q[-seq_len:] = q_\n",
    "#             qa[-seq_len:] = qa_\n",
    "        \n",
    "#         target_id = q[1:]\n",
    "#         label = qa[1:]\n",
    "\n",
    "#         x = np.zeros(self.max_seq-1, dtype=int)\n",
    "#         x = q[:-1].copy()\n",
    "#         x += (qa[:-1] == 1) * self.n_skill\n",
    "\n",
    "#         return x, target_id, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_indexes = list(group.index)[:TRAIN_SAMPLES]\n",
    "# valid_indexes = list(group.index)[TRAIN_SAMPLES:]\n",
    "# train_group = group[group.index.isin(train_indexes)]\n",
    "# valid_group = group[group.index.isin(valid_indexes)]\n",
    "# del group, train_indexes, valid_indexes\n",
    "# print(len(train_group), len(valid_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = SAKTDataset(train_group, n_skill, min_samples=MIN_SAMPLES, max_seq=MAX_SEQ)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "# valid_dataset = SAKTDataset(valid_group, n_skill, max_seq=MAX_SEQ)\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FFN(nn.Module):\n",
    "#     def __init__(self, state_size=200):\n",
    "#         super(FFN, self).__init__()\n",
    "#         self.state_size = state_size\n",
    "        \n",
    "#         self.lr1 = nn.Linear(state_size, state_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.lr2 = nn.Linear(state_size, state_size)\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.lr1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.lr2(x)\n",
    "#         return self.dropout(x)\n",
    "\n",
    "# def future_mask(seq_length):\n",
    "#     future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "#     return torch.from_numpy(future_mask)\n",
    "\n",
    "\n",
    "# class SAKTModel(nn.Module):\n",
    "#     def __init__(self, n_skill, max_seq=128, embed_dim=128, dropout_rate=0.2):\n",
    "#         super(SAKTModel, self).__init__()\n",
    "#         self.n_skill = n_skill\n",
    "#         self.embed_dim = embed_dim\n",
    "\n",
    "#         self.embedding = nn.Embedding(2*n_skill+1, embed_dim)\n",
    "#         self.pos_embedding = nn.Embedding(max_seq-1, embed_dim)\n",
    "#         self.e_embedding = nn.Embedding(n_skill+1, embed_dim)\n",
    "\n",
    "#         self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=dropout_rate)\n",
    "\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.layer_normal = nn.LayerNorm(embed_dim) \n",
    "\n",
    "#         self.ffn = FFN(embed_dim)\n",
    "#         self.pred = nn.Linear(embed_dim, 1)\n",
    "    \n",
    "#     def forward(self, x, question_ids):\n",
    "#         device = x.device        \n",
    "#         x = self.embedding(x)\n",
    "#         pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n",
    "\n",
    "#         pos_x = self.pos_embedding(pos_id)\n",
    "#         x = x + pos_x\n",
    "\n",
    "#         e = self.e_embedding(question_ids)\n",
    "\n",
    "#         x = x.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]\n",
    "#         e = e.permute(1, 0, 2)\n",
    "#         att_mask = future_mask(x.size(0)).to(device)\n",
    "#         att_output, att_weight = self.multi_att(e, x, x, attn_mask=att_mask)\n",
    "#         att_output = self.layer_normal(att_output + e)\n",
    "#         att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n",
    "\n",
    "#         x = self.ffn(att_output)\n",
    "#         x = self.layer_normal(x + att_output)\n",
    "#         x = self.pred(x)\n",
    "\n",
    "#         return x.squeeze(-1), att_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_fn(model, dataloader, optimizer, scheduler, criterion, device=\"cpu\"):\n",
    "#     model.train()\n",
    "\n",
    "#     train_loss = []\n",
    "#     num_corrects = 0\n",
    "#     num_total = 0\n",
    "#     labels = []\n",
    "#     outs = []\n",
    "\n",
    "#     for item in dataloader:\n",
    "#         x = item[0].to(device).long()\n",
    "#         target_id = item[1].to(device).long()\n",
    "#         label = item[2].to(device).float()\n",
    "#         target_mask = (target_id != 0)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         output, _, = model(x, target_id)\n",
    "#         loss = criterion(output, label)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "#         train_loss.append(loss.item())\n",
    "\n",
    "#         output = torch.masked_select(output, target_mask)\n",
    "#         label = torch.masked_select(label, target_mask)\n",
    "#         pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "        \n",
    "#         num_corrects += (pred == label).sum().item()\n",
    "#         num_total += len(label)\n",
    "\n",
    "#         labels.extend(label.view(-1).data.cpu().numpy())\n",
    "#         outs.extend(output.view(-1).data.cpu().numpy())\n",
    "\n",
    "#     acc = num_corrects / num_total\n",
    "#     auc = roc_auc_score(labels, outs)\n",
    "#     loss = np.mean(train_loss)\n",
    "\n",
    "#     return loss, acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def valid_fn(model, dataloader, criterion, device=\"cpu\"):\n",
    "#     model.eval()\n",
    "\n",
    "#     valid_loss = []\n",
    "#     num_corrects = 0\n",
    "#     num_total = 0\n",
    "#     labels = []\n",
    "#     outs = []\n",
    "\n",
    "#     for item in dataloader:\n",
    "#         x = item[0].to(device).long()\n",
    "#         target_id = item[1].to(device).long()\n",
    "#         label = item[2].to(device).float()\n",
    "#         target_mask = (target_id != 0)\n",
    "\n",
    "#         output, _, = model(x, target_id)\n",
    "#         loss = criterion(output, label)\n",
    "#         valid_loss.append(loss.item())\n",
    "\n",
    "#         output = torch.masked_select(output, target_mask)\n",
    "#         label = torch.masked_select(label, target_mask)\n",
    "#         pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "        \n",
    "#         num_corrects += (pred == label).sum().item()\n",
    "#         num_total += len(label)\n",
    "\n",
    "#         labels.extend(label.view(-1).data.cpu().numpy())\n",
    "#         outs.extend(output.view(-1).data.cpu().numpy())\n",
    "\n",
    "#     acc = num_corrects / num_total\n",
    "#     auc = roc_auc_score(labels, outs)\n",
    "#     loss = np.mean(valid_loss)\n",
    "\n",
    "#     return loss, acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = SAKTModel(n_skill, max_seq=MAX_SEQ, embed_dim=EMBED_DIM, dropout_rate=DROPOUT_RATE)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     optimizer, max_lr=MAX_LEARNING_RATE, steps_per_epoch=len(train_dataloader), epochs=EPOCHS\n",
    "# )\n",
    "\n",
    "# model.to(device)\n",
    "# criterion.to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_sakt(n_folds=0):\n",
    "#     best_auc = 0\n",
    "#     max_steps = 3\n",
    "#     step = 0\n",
    "#     for epoch in tqdm(range(EPOCHS)):\n",
    "#         loss, acc, auc = train_fn(model, train_dataloader, optimizer, scheduler, criterion, device)\n",
    "#         print(\"epoch - {}/{} train: - {:.3f} acc - {:.3f} auc - {:.3f}\".format(epoch+1, EPOCHS, loss, acc, auc))\n",
    "#         loss, acc, auc = valid_fn(model, valid_dataloader, criterion, device)\n",
    "#         print(\"epoch - {}/{} valid: - {:.3f} acc - {:.3f} auc - {:.3f}\".format(epoch+1, EPOCHS, loss, acc, auc))\n",
    "        \n",
    "#         if auc > best_auc:\n",
    "#             best_auc = auc\n",
    "#             step = 0\n",
    "#             torch.save(model.state_dict(), f\"sakt_model{n_folds}.pt\")\n",
    "#         else:\n",
    "#             step += 1\n",
    "#             if step >= max_steps:\n",
    "#                 break\n",
    "                \n",
    "#     return best_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17755881, 52) (2244119, 52)\n",
      "\n",
      "['content_id', 'timestamp', 'task_container_id', 'prior_question_elapsed_time', 'prior_question_had_explanation', 'bundle_id', 'part', 'content_cnt', 'content_inccnt', 'content_corrate', 'content_avg_time', 'content_expl_avg', 'new_content_corcnt', 'new_content_corrate', 'same_bundle_cnt', 'bundle_corrate', 'tags1', 'tags2', 'new_w2v', 'elapsed_time_u_avg', 'explanation_u_avg', 'timestamp_u_correct_recency', 'timestamp_u_incorrect_recency', 'timestamp_u_diff_0_1', 'timestamp_u_diff_1_2', 'timestamp_u_diff_2_3', 'task_container_id_back', 'user_cnt', 'user_corcnt', 'user_inccnt', 'user_corrate', 'user_diff_sum_corrate', 'user_diff_avg_corrate', 'user_roll10_corrate', 'user_roll30_corrate', 'user_diff_sum_time', 'user_diff_avg_time', 'user_past_content_cnt', 'user_part_cnt', 'user_part_corcnt', 'user_part_inccnt', 'user_part_corrate', 'user_part_diff_sum_corrate', 'user_part_diff_avg_corrate', 'user_part_roll5_corrate', 'user_part_roll15_corrate', 'user_w2v_corrate', 'user_tags1_corcnt', 'user_tags1_inccnt', 'user_tags1_corrate']\n",
      "50\n",
      "\n",
      "['content_id', 'prior_question_had_explanation', 'bundle_id', 'part', 'same_bundle_cnt', 'tags1', 'tags2', 'new_w2v', 'task_container_id_back']\n",
      "9\n",
      "\n",
      "answered_correctly\n",
      "18\n",
      "\n",
      "['user_part_cnt', 'bundle_corrate', 'user_part_inccnt', 'user_part_diff_avg_corrate', 'user_part_corrate', 'user_part_corcnt', 'user_diff_avg_corrate', 'content_expl_avg', 'user_part_roll15_corrate', 'explanation_u_avg', 'timestamp_u_diff_2_3', 'user_diff_avg_time', 'new_content_corcnt', 'prior_question_elapsed_time', 'content_corrate', 'elapsed_time_u_avg', 'user_roll10_corrate', 'user_roll30_corrate', 'user_past_content_cnt', 'timestamp_u_diff_0_1', 'user_tags1_inccnt', 'user_tags1_corrate', 'timestamp', 'user_inccnt', 'user_cnt', 'timestamp_u_incorrect_recency', 'user_diff_sum_time', 'content_cnt', 'timestamp_u_correct_recency', 'user_corcnt', 'task_container_id', 'user_w2v_corrate', 'user_diff_sum_corrate', 'user_part_diff_sum_corrate', 'user_part_roll5_corrate', 'user_tags1_corcnt', 'content_inccnt', 'user_corrate', 'new_content_corrate', 'content_avg_time', 'timestamp_u_diff_1_2']\n",
      "41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr.shape, val.shape)\n",
    "print('')\n",
    "for cols in [feature_cols,cat_feature_cols, target_col, num_feature_cols]:\n",
    "    print(cols)\n",
    "    print(len(cols))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-06 17:31:40.506847\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7554230\tbest: 0.7554230 (0)\ttotal: 1.89s\tremaining: 2d 4h 37m 55s\n",
      "100:\ttest: 0.7720553\tbest: 0.7720553 (100)\ttotal: 2m 32s\tremaining: 1d 17h 47m 52s\n"
     ]
    }
   ],
   "source": [
    "#xgb\n",
    "# xgb_auc, xgb_model = train_xgb(tr, val, feature_cols, target_col, num_iter=10000)\n",
    "#lgbm\n",
    "# lgb_auc, lgb_model = train_lgbm(tr, val, feature_cols, cat_feature_cols, target_col, num_iter=10000)\n",
    "#ctb\n",
    "ctb_auc, ctb_model = train_ctb(tr, val, feature_cols, target_col, num_iter=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-06 21:39:18.901822\n"
     ]
    }
   ],
   "source": [
    "finish = time.time()\n",
    "print(datetime.datetime.now())\n",
    "# print(finish - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7998176581697033\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "#     xgb_auc,\n",
    "#     lgb_auc,\n",
    "    ctb_auc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save models\n",
    "# with open(f'xgb_model.pkl', mode='wb') as fp:\n",
    "#     pickle.dump(xgb_model, fp)\n",
    "# with open(f'lgb_gcp_2.pkl', mode='wb') as fp:\n",
    "#     pickle.dump(lgb_model, fp, protocol=4)\n",
    "#catboostだけ特殊\n",
    "ctb_model.save_model('ctb_model_20mil_100th.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoost at 0x7fa53d5e4ad0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load\n",
    "with open(f'ctb_model_20mil_100th.pkl', mode='wb') as fp:\n",
    "    pickle.dump(ctb_model, fp)\n",
    "    \n",
    "with open('ctb_model_20mil_100th.pkl', mode='rb') as fp:\n",
    "    ctb_model_restore = pickle.load(fp)\n",
    "#catboost\n",
    "ctb_model_restore = ctb.CatBoost()\n",
    "ctb_model_restore.load_model('ctb_model_20mil_100th.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# print(xgb_model.best_iteration == xgb_model_restore.best_iteration)\n",
    "# print(lgb_model.best_iteration == lgb_model_restore.best_iteration)\n",
    "print(ctb_model == ctb_model_restore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8639a82236104e93bac6fdc59d60be7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sakt_auc = train_sakt(n_folds=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = TestDataset(test, num_features, cat_features)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, \n",
    "#                              num_workers=4, pin_memory=True)\n",
    "# model = TabularNN(cfg)\n",
    "# model.load_state_dict(torch.load(f\"fold{fold_num}_seed{seed}.pth\"))\n",
    "# model.to(device)\n",
    "# predictions = inference_fn(test_loader, model, device)\n",
    "\n",
    "# # del\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
